{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import SequenceMatcher\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Define the directory paths\n",
    "data_dir = r'D:\\Research\\Python\\Data\\WIP\\Google search data Preprocessed'\n",
    "output_dir = r'D:\\Research\\Python\\Data\\WIP\\Final Association Rated'\n",
    "\n",
    "# Load keywords_df\n",
    "keywords_df = pd.read_csv(os.path.join(data_dir, 'Search_Terms_Hashtags.csv'))\n",
    "\n",
    "# Convert \"Spike Date\" column to datetime format\n",
    "keywords_df['Spike Date'] = pd.to_datetime(keywords_df['Spike Date'], format='%m/%d/%Y')\n",
    "\n",
    "# List all files in the directory\n",
    "file_list = [file for file in os.listdir(data_dir) if file.endswith('_preprocessed.csv')]\n",
    "\n",
    "# Initialize an empty list to store the top rows\n",
    "top_rows = []\n",
    "\n",
    "# Process each file\n",
    "for file in file_list:\n",
    "    if file != 'Search_Terms_Hashtags_preprocessed.csv':\n",
    "        # Extract date from the file name\n",
    "        date_str = file.split('_')[2]  # Assuming the date is at the third position\n",
    "        date = pd.to_datetime(date_str, format='%m-%d-%Y')  # Corrected format\n",
    "        \n",
    "        # Load the dataset and drop null rows from 'Combined_text_preprocessed'\n",
    "        df = pd.read_csv(os.path.join(data_dir, file)).dropna(subset=['Combined_text_preprocessed'])\n",
    "        \n",
    "        # Find matching row in keywords_df based on Spike Date\n",
    "        matching_row = keywords_df[keywords_df['Spike Date'] == date]\n",
    "        \n",
    "        if not matching_row.empty:\n",
    "            # Get the Terms column\n",
    "            terms = matching_row['Terms'].iloc[0]  # Extract the string\n",
    "            \n",
    "            # Calculate cosine similarity for each row\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix_terms = vectorizer.fit_transform([terms])\n",
    "            cosine_similarity_scores = cosine_similarity(tfidf_matrix_terms, vectorizer.transform(df['Combined_text_preprocessed']))\n",
    "            \n",
    "            # Calculate SequenceMatcher similarity for each row\n",
    "            sequence_similarity_scores = []\n",
    "            for index, row in df.iterrows():\n",
    "                combined_text = row['Combined_text_preprocessed']\n",
    "                sequence_similarity = SequenceMatcher(None, terms, combined_text).ratio()\n",
    "                sequence_similarity_scores.append(sequence_similarity)\n",
    "            \n",
    "            # Calculate Levenshtein distance for each row\n",
    "            levenshtein_distances = []\n",
    "            for index, row in df.iterrows():\n",
    "                combined_text = row['Combined_text_preprocessed']\n",
    "                levenshtein_distance = fuzz.ratio(terms, combined_text)\n",
    "                # Normalize Levenshtein distance to a scale between 0 and 1\n",
    "                normalized_levenshtein = 1 - (levenshtein_distance / 40)  \n",
    "                levenshtein_distances.append(normalized_levenshtein)\n",
    "            \n",
    "            # Add the scores to the dataframe\n",
    "            df['Cosine_Similarity'] = cosine_similarity_scores[0]\n",
    "            df['Sequence_Similarity'] = sequence_similarity_scores\n",
    "            df['Levenshtein_Distance'] = levenshtein_distances\n",
    "            \n",
    "            # Calculate the overall score by summing up the scores\n",
    "            df['Overall_Score'] = df['Cosine_Similarity'] + df['Sequence_Similarity'] + df['Levenshtein_Distance']\n",
    "            \n",
    "            # Select the top ten rows by Overall_Score and add to the list\n",
    "            top_rows.append(df.nlargest(10, '   '))\n",
    "            \n",
    "            # Save the modified dataframe with a new name\n",
    "            new_filename = file.replace('_preprocessed.csv', f'_scores_{date_str}.csv')\n",
    "            df.to_csv(os.path.join(output_dir, new_filename), index=False)\n",
    "        else:\n",
    "            print(f\"No matching row found for {file}\")\n",
    "\n",
    "# Concatenate the top rows into a single dataframe\n",
    "final_df = pd.concat(top_rows, ignore_index=True)\n",
    "\n",
    "# Save the concatenated dataframe\n",
    "final_df.to_csv(os.path.join(output_dir, 'final_association_rated.csv'), index=False)\n",
    "\n",
    "print(\"Process completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
